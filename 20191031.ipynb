{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20191031.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssujinK/SEMINAR-IN-adv.-STAT./blob/master/20191031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBIOesOuQRyn",
        "colab_type": "code",
        "outputId": "2896b77d-0526-423c-a672-eb5b2f42232b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "  # Colab only\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwtJk1O2QYKL",
        "colab_type": "code",
        "outputId": "22d26eb4-4816-4371-a63d-d4e715b33869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras를 임포트합니다\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 헬퍼(helper) 라이브러리를 임포트합니다\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## 패션 MNIST 데이터셋 임포트하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAsbpxhfQYRv",
        "colab_type": "code",
        "outputId": "ddeac17f-979d-4c29-b13d-9f678418aadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgNcTLngQYUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Brm0b_KACaWX"
      },
      "source": [
        "## 데이터 탐색\n",
        "\n",
        "모델을 훈련하기 전에 데이터셋 구조를 살펴보죠. 다음 코드는 훈련 세트에 60,000개의 이미지가 있다는 것을 보여줍니다. 각 이미지는 28x28 픽셀로 표현됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb9UzqIQQYWn",
        "colab_type": "code",
        "outputId": "999972dc-99c6-4823-ae8a-6ca87371deaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSErhzZuQcq6",
        "colab_type": "code",
        "outputId": "feb8cd1b-18fe-400c-aa48-f55105926878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Zue6reQYZC",
        "colab_type": "code",
        "outputId": "1eb59bb2-32dc-4bb6-a2b1-3826f4d7556e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTzfFNBqQYbU",
        "colab_type": "code",
        "outputId": "c0d3d695-bcb0-49cd-d83f-375fb4870806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpQLZ6CyQfg6",
        "colab_type": "code",
        "outputId": "58f2f908-f5f0-4b4f-de12-f57f2e38eb92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "네트워크를 훈련하기 전에 데이터를 전처리해야 합니다. 훈련 세트에 있는 첫 번째 이미지를 보면 픽셀 값의 범위가 0~255 사이라는 것을 알 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLAcfe2jQfje",
        "colab_type": "code",
        "outputId": "f88b8c82-d9f5-44b9-af26-7dce2a122ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD4CAYAAACE9dGgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSmIiUmhKYiJSaEpiIlJoSmIiUmj/H4BqExLuMX2fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GTiF6UTQfmM",
        "colab_type": "code",
        "outputId": "0a239cf4-614b-4533-d1c0-ae07641db6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnPVUHkqQfou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViI70t0zQfrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 이미지 사이즈 조정\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ILCa3yXHV-qj"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "네트워크를 훈련하기 전에 데이터를 전처리해야 합니다. 훈련 세트에 있는 첫 번째 이미지를 보면 픽셀 값의 범위가 0~255 사이라는 것을 알 수 있습니다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### 층 설정\n",
        "\n",
        "신경망의 기본 구성 요소는 *층*(layer)입니다. 층은 주입된 데이터에서 표현을 추출합니다. 아마도 문제를 해결하는데 더 의미있는 표현이 추출될 것입니다.\n",
        "\n",
        "대부분 딥러닝은 간단한 층을 연결하여 구성됩니다. `tf.keras.layers.Dense`와 같은 층들의 가중치(parameter)는 훈련하는 동안 학습됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qij_OpvjQftc",
        "colab_type": "code",
        "outputId": "9c590b7f-54de-4a8a-a3a2-26e149c77def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "## 모형만들기\n",
        "### tensorflow.keras 로 불러와야 사용 가능(tensorflow가 아닌 keras가 로딩되어 오류날 수 있음)\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import Sequential\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape = (28,28,1),\n",
        "filters = 32,\n",
        "kernel_size = (3,3),\n",
        "strides = (1,1),\n",
        "padding='same',\n",
        "activation ='relu') )\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               3211392   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 3,213,002\n",
            "Trainable params: 3,213,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-mSa6YOSknK",
        "colab_type": "code",
        "outputId": "ecb32cc0-cd16-40f8-aab8-3cc396fde8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "# practice\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(input_shape = (28,28,1),filters = 16,kernel_size = (5,5),\n",
        "                 strides = (1,1), padding='valid', activation ='relu') )\n",
        "model.add( MaxPooling2D(pool_size = (2,2), strides = (1,1), padding = 'valid') )\n",
        "model.add(Conv2D(input_shape = (28,28,1),filters = 32,kernel_size = (5,5),\n",
        "                 strides = (1,1), padding='valid',activation ='relu') )\n",
        "model.add( MaxPooling2D(pool_size = (2,2), strides = (1,1), padding = 'valid') )\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 24, 24, 16)        416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 23, 23, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 19, 19, 32)        12832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 18, 18, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 10368)             0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               1327232   \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,341,770\n",
            "Trainable params: 1,341,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z28l2UgGQm90",
        "colab_type": "code",
        "outputId": "7b21e91d-78e4-4b67-da38-bc9d88765959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "# practice: 아래의 LeNet-5 코드에서 틀린 부분 찾아서 고쳐보세요.\n",
        "# AveragePooing에서 strides size 없애줌\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh'\n",
        ", input_shape=(28,28,1), padding='same'))\n",
        "model.add(AveragePooling2D(pool_size=2, padding='valid'))\n",
        "model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1),\n",
        "activation='tanh'\n",
        ", padding='valid'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1),\n",
        "activation='tanh'\n",
        ", padding='valid'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(84, activation='tanh'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_32 (Conv2D)           (None, 28, 28, 6)         156       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_10 (Averag (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_11 (Averag (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 1, 1, 120)         48120     \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHzgVUPqe2t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Skip Connection Practice\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Activation, Conv2D, BatchNormalization, GlobalAveragePooling2D, Dropout, Dense, add\n",
        "from tensorflow.keras.activations import relu, softmax\n",
        "from tensorflow.keras import regularizers\n",
        "def block(n_output, upscale = False) :\n",
        "  def f(x) :\n",
        "    h = BatchNormalization()(x)\n",
        "    h = Activation(relu)(h)\n",
        "    h = Conv2D(kernel_size = 3, filters = n_output, padding = 'same')(h)\n",
        "    h = BatchNormalization()(h)\n",
        "    h = Activation(relu)(h)\n",
        "    h = Conv2D(kernel_size = 3, filters = n_output, padding = 'same')(h)\n",
        "    if upscale :\n",
        "      f = Conv2D(kernel_size = 1, filters=n_output, strides=1, padding='same')(x)\n",
        "    else :\n",
        "      f = x\n",
        "    return add([f,h])\n",
        "  return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNPwH7v1e-H4",
        "colab_type": "code",
        "outputId": "74cb07a6-e0d1-4321-d315-17724abb79b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Skip Connection Practice\n",
        "\n",
        "input_shape = (28,28,1)\n",
        "input_tensor = Input(input_shape)\n",
        "x = Conv2D(kernel_size = 3, filters = 16, strides = 1, padding = 'same')(input_tensor)\n",
        "x = block(16)(x)\n",
        "x = block(16)(x)\n",
        "x = block(32, upscale = True)(x)\n",
        "x = block(32)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(relu)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(units=10, kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = Activation(softmax)(x)\n",
        "model = Model(inputs=input_tensor, outputs=x)\n",
        "model.summary()\n",
        "\n",
        "# f(f(f(x)))의 형태"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 28, 28, 16)   160         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 28, 28, 16)   64          conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 28, 28, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 28, 28, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 28, 28, 16)   64          conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 28, 28, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 28, 28, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 28, 28, 16)   0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 28, 28, 16)   64          add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 28, 28, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 28, 28, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 28, 28, 16)   64          conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 28, 28, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 28, 28, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 28, 28, 16)   0           add[0][0]                        \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 28, 28, 16)   64          add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 28, 28, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 28, 28, 32)   4640        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 28, 28, 32)   128         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 28, 28, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 28, 28, 32)   544         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 28, 28, 32)   9248        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 28, 28, 32)   0           conv2d_42[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 28, 28, 32)   128         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 28, 28, 32)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 28, 28, 32)   9248        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 28, 28, 32)   128         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 28, 28, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 28, 28, 32)   9248        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 28, 28, 32)   0           add_2[0][0]                      \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 28, 28, 32)   128         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 28, 28, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 32)           0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32)           0           global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 10)           330         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 10)           0           dense_24[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 43,530\n",
            "Trainable params: 43,114\n",
            "Non-trainable params: 416\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "이 네트워크의 첫 번째 층인 `tf.keras.layers.Flatten`은 2차원 배열(28 x 28 픽셀)의 이미지 포맷을 28 * 28 = 784 픽셀의 1차원 배열로 변환합니다. 이 층은 이미지에 있는 픽셀의 행을 펼쳐서 일렬로 늘립니다. 이 층에는 학습되는 가중치가 없고 데이터를 변환하기만 합니다.\n",
        "\n",
        "픽셀을 펼친 후에는 두 개의 `tf.keras.layers.Dense` 층이 연속되어 연결됩니다. 이 층을 밀집 연결(densely-connected) 또는 완전 연결(fully-connected) 층이라고 부릅니다. 첫 번째 `Dense` 층은 128개의 노드(또는 뉴런)를 가집니다. 두 번째 (마지막) 층은 10개의 노드의 *소프트맥스*(softmax) 층입니다. 이 층은 10개의 확률을 반환하고 반환된 값의 전체 합은 1입니다. 각 노드는 현재 이미지가 10개 클래스 중 하나에 속할 확률을 출력합니다.\n",
        "\n",
        "### 모델 컴파일\n",
        "\n",
        "모델을 훈련하기 전에 필요한 몇 가지 설정이 모델 *컴파일* 단계에서 추가됩니다:\n",
        "\n",
        "* *손실 함수*(Loss function)-훈련 하는 동안 모델의 오차를 측정합니다. 모델의 학습이 올바른 방향으로 향하도록 이 함수를 최소화해야 합니다.\n",
        "* *옵티마이저*(Optimizer)-데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정합니다.\n",
        "* *지표*(Metrics)-훈련 단계와 테스트 단계를 모니터링하기 위해 사용합니다. 다음 예에서는 올바르게 분류된 이미지의 비율인 *정확도*를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb5dwpOFQYdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## 모델 훈련\n",
        "\n",
        "신경망 모델을 훈련하는 단계는 다음과 같습니다:\n",
        "\n",
        "1. 훈련 데이터를 모델에 주입합니다-이 예에서는 `train_images`와 `train_labels` 배열입니다.\n",
        "2. 모델이 이미지와 레이블을 매핑하는 방법을 배웁니다.\n",
        "3. 테스트 세트에 대한 모델의 예측을 만듭니다-이 예에서는 `test_images` 배열입니다. 이 예측이 `test_labels` 배열의 레이블과 맞는지 확인합니다.\n",
        "\n",
        "훈련을 시작하기 위해 `model.fit` 메서드를 호출하면 모델이 훈련 데이터를 학습합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVJOE7k6Qm7V",
        "colab_type": "code",
        "outputId": "1f683515-8af1-4d6f-86de-520c2441c6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "60000/60000 [==============================] - 81s 1ms/sample - loss: 0.3701 - accuracy: 0.8676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb057760f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K0yA-rgQ37c",
        "colab_type": "text"
      },
      "source": [
        "## **tf.keras.layers.MaxPooling2D**\n",
        "\n",
        "\n",
        "MaxPooling2D(pool_size = pool 하는 차원지정 ex: (2,2) ,  \n",
        "strides = 몇 칸씩 이동시킬지?  \n",
        "padding = “valid” or “same”(go with 0 padding)\n",
        "    # valid는 무시하고 넘어감, same은 무시하지 않음 \n",
        "data_format = “channels_last”, “channels_first”)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvZvVfTXYoq6",
        "colab_type": "text"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Jm7NcaQnAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wow7UZcbQnCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq-2n6-6QnE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbJNIe0QnHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM0x_95viDlx",
        "colab_type": "text"
      },
      "source": [
        "# **HW 3**\n",
        "\n",
        "#### 1. Fashion Mnist 데이터를 잘 적합시키는 CNN 기반 모형 찾기\n",
        "https://www.tensorflow.org/tutorials/keras/classification\n",
        "#### 2. Mnist 데이터를 잘 적합시키는 CNN 기반 모형 찾기\n",
        "https://www.tensorflow.org/tutorials/quickstart/beginner\n",
        "  \n",
        "\n",
        "competition의 관심있는 technical paper 찾아보기"
      ]
    }
  ]
}